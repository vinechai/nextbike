name: Scrape Prague Nextbike

on:
  schedule:
    - cron: "*/5 * * * *"   # run every 5 minutes
  workflow_dispatch:         # allow manual run

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run scraper
        run: |
          python scrape_prague.py

      # --------------------
      # Upload all parquet files to Backblaze
      # --------------------
      - name: Upload to Backblaze B2
        run: |
          pip install boto3

          python - <<EOF
          import boto3
          import os
          from pathlib import Path

          s3 = boto3.client(
              "s3",
              endpoint_url=f"https://${{ secrets.B2_ENDPOINT }}",
              aws_access_key_id="${{ secrets.B2_KEY_ID }}",
              aws_secret_access_key="${{ secrets.B2_APPLICATION_KEY }}",
          )

          bucket = "${{ secrets.B2_BUCKET_NAME }}"
          data_dir = Path("data")

          # upload every file in data/
          for file in data_dir.glob("*"):
              s3.upload_file(str(file), bucket, file.name)
              print("Uploaded:", file.name)
          EOF
